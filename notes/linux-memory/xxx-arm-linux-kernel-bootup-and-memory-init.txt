-------------------------------------------------------------------------------------------------
https://www.cnblogs.com/arnoldlu/category/1132616.html !!!!!!!!!!very very good
-------------------------------------------------------------------------------------------------

1,每当切换的时候，需要切换进程地址空间（切换页表，进程的页表信息位于其mm_struct的pgd成员中），用户线程到用户线程的切换当然没有问题，但是用户线程和内核线程之间的切换就会有问题，因为内核线程没有memory descriptor，怎么办？只能是借用了，因此，在进程描述符（task_struct）中有一个active_mm的成员，表示该进程（内核线程）当前正在使用的memory descriptor（注意：“使用”并不表示“拥有”）。
对于普通用户进程，它拥有自己的memory descriptor，因此进程描述符的active_mm和mm成员指向同一个memory descriptor（自己有memory descriptor，当然使用自己的了）。对于内核线程，它不可能”拥有“memory descriptor（mm等于NULL），因此active_mm需要借用其他进程的memory descriptor，可以参考下面的代码（代码来自4.1.10）：
static inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
           struct task_struct *next)
{
    struct mm_struct *mm, *oldmm;

    prepare_task_switch(rq, prev, next);

    mm = next->mm;
    oldmm = prev->active_mm;
    /*
     * For paravirt, this is coupled with an exit in switch_to to
     * combine the page table reload and the switch backend into
     * one hypercall.
     */
    arch_start_context_switch(prev);

    if (!mm) {－－－－－－－－－－－－－－－－切换到一个内核线程（next的mm成员为NULL）
        next->active_mm = oldmm;－－－－－－－借用上一个的使用的memory descriptor
        atomic_inc(&oldmm->mm_count);
        enter_lazy_tlb(oldmm, next);
    } else－－－－－－－－－－－－－－－－－－－切换到普通进程
        switch_mm(oldmm, mm, next);

    if (!prev->mm) {
        prev->active_mm = NULL;－－－－－－－－－如果是从一个内核线程切换到某个其他进程，那么借用期结束
        rq->prev_mm = oldmm;
    }
    /*
     * Since the runqueue lock will be released by the next
     * task (which is an invalid locking op but in the case
     * of the scheduler it's an obvious special-case), so we
     * do an early lockdep release here:
     */
    spin_release(&rq->lock.dep_map, 1, _THIS_IP_);

    context_tracking_task_switch(prev, next);
    /* Here we just switch the register state and the stack. */
    switch_to(prev, next, prev);
    barrier();

    return finish_task_switch(prev);
}
－－－－－－－－－－－－－－－－－－－－－－－－－－
讲了这么多，似乎还是没有到init_mm，呵呵，那么init_mm到底是属于哪一个进程呢？其实init_mm不属于任何的进程或者内核线程。当然，在启动阶段，swapper进程（idle进程，pid等于0的那个进程）曾经借用国init_mm，但是初始化完成，调度器正常运作之后（至少发生了一次进程切换涉及到了idle进程），init_mm不和任何的进程相关了。
2,内核进程的mm成员为NULL，需要使用active_mm成员。
3,arch/arm/mm/proc-v7-2level.S
/*
 *  cpu_v7_switch_mm(pgd_phys, tsk)
 *
 *  Set the translation table base pointer to be pgd_phys
 *
 *  - pgd_phys - physical address of new TTB
 *
 *  It is assumed that:
 *  - we are not using split page tables
 *
 *  Note that we always need to flush BTAC/BTB if IBE is set
 *  even on Cortex-A8 revisions not affected by 430973.
 *  If IBE is not set, the flush BTAC/BTB won't do anything.
 */
ENTRY(cpu_v7_switch_mm)
#ifdef CONFIG_MMU
    mmid    r1, r1              @ get mm->context.id
    ALT_SMP(orr r0, r0, #TTB_FLAGS_SMP)
    ALT_UP(orr  r0, r0, #TTB_FLAGS_UP)
#ifdef CONFIG_PID_IN_CONTEXTIDR
    mrc p15, 0, r2, c13, c0, 1      @ read current context ID
    lsr r2, r2, #8          @ extract the PID
    bfi r1, r2, #8, #24         @ insert into new context ID
#endif
#ifdef CONFIG_ARM_ERRATA_754322
    dsb
#endif
    mcr p15, 0, r1, c13, c0, 1      @ set context ID
    isb
    mcr p15, 0, r0, c2, c0, 0       @ set TTB 0
    isb
#endif
    bx  lr
ENDPROC(cpu_v7_switch_mm)
4, 另外在操作896M以上的虚拟内存时，只会更改swapper_pg_dir的映射信息，当别的进程访问到这些页面的时候会发生缺页，在缺页处理中会与swapper_pg_dir同步。


QQ群中的MBeginner问了这样的一个问题：
如果进程A、B都访问了某个虚拟地址x，此时A、B的页表都与init_mm同步后，然后进程A修改了地址x的映射关系，修改的是init_mm的页表，此时如果不更新进程B的页表，那么进程B再次访问地址X的时候会认为当前页表是有效的，不会发生缺页中断，从而导致实际上访问的是错误的物理地址， 这种情况可能出现么？

整理大家的回答如下：
这种情况不会发生。
首先，内核空间可以分成下面几种：
1、    directed mapped memory region，就是mapping到main memory的那段虚拟地址空间，VM和PM之间有一个固定的offset。这个memory region的页表是初始化建立的，是不会动态变化的。
2、    permanent kernel mapping、temporary kernel mapping和noncontiguous memory allocation（就是大家熟悉的vmalloc适用的memory region）。这个段的页表可以被更新。
因此，我相信大家说到页表同步问题都是指的第二段memory region的页表同步问题。

关于你说的场景，我分步描述如下：
Step 1：A进程调用vmalloc分配了一段虚拟内存，首地址是x。这时候，所有进程的页表并没有建立关于x的项目，唯一完整建立x地址段的页表是init_mm的PGD。

Step 2：当进程A、B都访问了某个虚拟地址x，此时A、B的页表都与init_mm同步后，A和B也都建立好了关于x这个虚拟地址段的页表。但是，这里需要强调的是：对于x虚拟地址段，A进程和B进程有不同的PGD，但是它们PGD中关于x地址段的entry们都是指向相同的PMD。当然PMD entry指向的PTE也是相同的。换句话说，对于x地址段，所有进程还有init mm而言，它们的PMD和PTE都是共享的。

Step 3：进程A调用vfree函数释放了地址x，当然操作的依然是init mm的PGD以及其下级的页表们。需要说明的是：所有根是init mm PGD的那些PMD和PTE的内存不会回收，因此x地址段的PMD和PTE页表本身的内存并不会释放，vfree只是将x地址段对应的页表项全部清除。

Step 4：进程B访问x地址段的时候，PGD entry指向了大家共享的PMD，PMD的entry指向了大家共享的PTE，但是，PTE中的具体的entry已经被清除，因此产生page fault







mm_struct中有两个成员和用户地址空间相关：
1、mmap（双向链表）
2、mm_rb（红黑树）
每当进程分配一段用户地址空间（注意：和内核空间无关），都会分配一个vm_area_struct，被挂入上面的链表和红黑树中。因此init_mm中的红黑树和链表都是没有任何意义的，因为它主要是for内核地址空间的。

我们再回到内核地址空间来看，就Translation table而言，和内核地址空间相关的ranslation table包括：
1、Master kernel PGD
2、各个用户进程的PGD
3、内核空间的PMD和PTE
在上面的ranslation table而言，所有的用户空间进程以及内核线程都共享item 3. 各个用户进程拥有自己的PGD，内核线程没有自己的PGD，只是借用某个用户进程的PGD（借用哪一个是看缘分，上面说了，是和进程切换相关）。Master kernel PGD不属于任何的用户进程或者内核线程，它就是一个共用的模板而已。当申请了内核空间的虚拟地址段的时候，内核只是修改了Master kernel PGD以及其child translation table的内容，也就是说仅仅是在模板上设立好了Translation table，而各个进程的Translation table还都是空的，这些是在真正访问的时候，发生异常，在异常处理流程中，根据模板中的数据建立自己的页表数据。

-------------------------------------------------------------------------------------------------
32bit的Linux采用三级映射：PGD-->PMD-->PTE，64bit的Linux采用四级映射：PGD-->PUD-->PMD-->PTE，多了个PUD。

缩写是PGD：Page Global Directory、PUD：Page Upper Directory、PMD：Page Middle Directory、PTE：Page Table Entry。

在ARM32 Linux采用两层映射，省略了PMD，除非在定义了CONFIG_ARM_LPAE才会使用3级映射。
-------------------------------------------------------------------------------------------------
第一级的页表的页目录表项用pgd表示，中间级的页表的页目录表项用pud表示（arm架构其实不需要），第三级的页表的页目录表项用pmd表示（由于中间pud是空的，所以pgd=pmd），
a3 register
page global directory
page upper directory
page middle directory
page table
page
-------------------------------------------------------------------------------------------------
在开发板tqs3c2440中，SDRAM连接到内存控制器的Bank6中，它的开始内存地址是0x30000000，大小为64M，即0x20000000。 ARM Linux kernel将SDRAM的开始地址定义为PHYS_OFFSET。经bootloader加载kernel并由自解压部分代码运行后，最终kernel被放置到KERNEL_RAM_PADDR（=PHYS_OFFSET + TEXT_OFFSET，即0x30008000）地址上的一段内存，经此放置后，kernel代码以后均不会被移动。



在进入kernel代码前，即bootloader和自解压缩阶段，ARM未开启MMU功能。因此kernel启动代码一个重要功能是设置好相应的页表，并开启MMU功能。为了支持MMU功能，kernel镜像中的所有符号，包括代码段和数据段的符号，在链接时都生成了它在开启MMU时，所在物理内存地址映射到的虚拟内存地址。



以arm kernel第一个符号（函数）stext为例，在编译链接，它生成的虚拟地址是0xc0008000，而放置它的物理地址为0x30008000（还记得这是PHYS_OFFSET+TEXT_OFFSET吗？）。实际上这个变换可以利用简单的公式进行表示：va = pa – PHYS_OFFSET + PAGE_OFFSET。Arm linux最终的kernel空间的页表，就是按照这个关系来建立。



之所以较早提及arm linux 的内存映射，原因是在进入kernel代码，里面所有符号地址值为清一色的0xCXXXXXXX地址，而此时ARM未开启MMU功能，故在执行stext函数第一条执行时，它的PC值就是stext所在的内存地址（即物理地址，0x30008000）。因此，下面有些代码，需要使用地址无关技术。
-------------------------------------------------------------------------------------------------
LL:low level log print
--------------------------------------------------------------------------------------------------
/*
 * The following fragment of code is executed with the MMU on in MMU mode,
 * and uses absolute addresses; this is not position independent.
 *
 *  r0  = cp#15 control register
 *  r1  = machine ID
 *  r2  = atags/dtb pointer
 *  r9  = processor ID
 */
    __INIT
__mmap_switched:
    adr r3, __mmap_switched_data<<-------------------------------------------------------

    ldmia   r3!, {r4, r5, r6, r7}
    cmp r4, r5              @ Copy data segment if needed
1:  cmpne   r5, r6
    ldrne   fp, [r4], #4
    strne   fp, [r5], #4
    bne 1b

    mov fp, #0              @ Clear BSS (and zero fp)
1:  cmp r6, r7
    strcc   fp, [r6],#4
    bcc 1b

 ARM(   ldmia   r3, {r4, r5, r6, r7, sp})
 THUMB( ldmia   r3, {r4, r5, r6, r7}    )
 THUMB( ldr sp, [r3, #16]       )
    str r9, [r4]            @ Save processor ID
    str r1, [r5]            @ Save machine type
    str r2, [r6]            @ Save atags pointer
    cmp r7, #0
    strne   r0, [r7]            @ Save control register values
    b   start_kernel
ENDPROC(__mmap_switched)
    .align  2
    .type   __mmap_switched_data, %object
__mmap_switched_data:-------------------------------------------------------------------->>
    .long   __data_loc          @ r4
    .long   _sdata              @ r5
    .long   __bss_start         @ r6
    .long   _end                @ r7
    .long   processor_id            @ r4
    .long   __machine_arch_type     @ r5
    .long   __atags_pointer         @ r6
#ifdef CONFIG_CPU_CP15
    .long   cr_alignment            @ r7
#else
    .long   0               @ r7
#endif
    .long   init_thread_union + THREAD_START_SP @ sp////////////////////////////////////////////#0 task stack, why "init_thread_union + THREAD_START_SP" ? because stack point grow down!!!!
    .size   __mmap_switched_data, . - __mmap_switched_data
---------------------------------------------------------------------------
Breakpoint 1, create_mapping (md=0x80b01f3c) at arch/arm/mm/mmu.c:961
961	{
#0  create_mapping (md=0x80b01f3c) at arch/arm/mm/mmu.c:961
#1  0x80a08588 in iotable_init (io_desc=0x80b01f3c, nr=0) at arch/arm/mm/mmu.c:1008
#2  0x80a0890c in debug_ll_io_init () at arch/arm/mm/mmu.c:1132
#3  0x80a08f10 in devicemaps_init (mdesc=0x80a33028 <__mach_desc_VEXPRESS_DT>) at arch/arm/mm/mmu.c:1423
#4  0x80a09490 in paging_init (mdesc=0x80a33028 <__mach_desc_VEXPRESS_DT>) at arch/arm/mm/mmu.c:1655
#5  0x80a044ec in setup_arch (cmdline_p=0x80b01fd4) at arch/arm/kernel/setup.c:1133
#6  0x80a00c38 in start_kernel () at init/main.c:778
#7  0x00000000 in ?? ()
Backtrace stopped: previous frame identical to this frame (corrupt stack?)

Breakpoint 3, mem_init () at arch/arm/mm/init.c:472
472	{
#0  mem_init () at arch/arm/mm/init.c:472
#1  0x80a01100 in mm_init () at init/main.c:738
#2  start_kernel () at init/main.c:812
#3  0x00000000 in ?? ()
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
---------------------------------------------------------------------------
start_kernel()
    |---->page_address_init()
    |     考虑支持高端内存
    |     业务：初始化page_address_pool链表；
    |          将page_address_maps数组元素按索引降序插入
    |          page_address_pool链表;
    |          初始化page_address_htable数组.
    |
    |---->setup_arch(&command_line);
    |     初始化特定体系结构的内容
        |---->arm64_memblock_init( );               [参见memblock和bootmem]
        |     初始化引导阶段的内存分配器memblock
        |
        |---->paging_init();                         [参见分页机制初始化paging_init]
        |     分页机制初始化
        |
        |---->bootmem_init();                       [与build_all_zonelist共同完成内存数据结构的初始化]
        |       初始化内存数据结构包括内存节点和内存域
        |
    |---->setup_per_cpu_areas();
    |     为per-CPU变量分配空间
    |
    |---->build_all_zonelist()                      [bootmem_init初始化数据结构, 该函数初始化zonelists]
    |     为系统中的zone建立后备zone的列表.
    |     所有zone的后备列表都在
    |     pglist_data->node_zonelists[0]中;
    |
    |     期间也对per-CPU变量boot_pageset做了初始化.
    |
    |---->page_alloc_init()
         |---->hotcpu_notifier(page_alloc_cpu_notifier, 0);
         |     不考虑热插拔CPU
         |
    |---->pidhash_init()
    |     详见下文.
    |     根据低端内存页数和散列度，分配hash空间，并赋予pid_hash
    |
    |---->vfs_caches_init_early()
          |---->dcache_init_early()
          |     dentry_hashtable空间，d_hash_shift, h_hash_mask赋值；
          |     同pidhash_init();
          |     区别:
          |         散列度变化了（13 - PAGE_SHIFT）;
          |         传入alloc_large_system_hash的最后参数值为0;
          |
          |---->inode_init_early()
          |     inode_hashtable空间，i_hash_shift, i_hash_mask赋值；
          |     同pidhash_init();
          |     区别:
          |         散列度变化了（14 - PAGE_SHIFT）;
          |         传入alloc_large_system_hash的最后参数值为0;
          |

4.3 bootmem_init初始化内存的基础数据结构(结点pg_data, 内存域zone, 页面page)
bootmem_init(void)
    |---->min = PFN_UP(memblock_start_of_DRAM());
    |---->max = PFN_DOWN(memblock_end_of_DRAM());
    |
    |
    |---->arm64_numa_init();
    |     支持numa架构
    |---->arm64_numa_init();
    |     支持numa架构
    |
    |
    |---->zone_sizes_init(min, max);
        来初始化节点和管理区的一些数据项
        |
        |---->free_area_init_node
        |   初始化内存节点
        |
        |
            |---->free_area_init_core初始化zone
                |
                |
                |---->memmap_init初始化page页面
                |
                |
    |
    |---->memblock_dump_all();
    |   初始化完成, 显示memblock的保留的所有内存信息

4.4 build_all_zonelists初始化每个内存节点的zonelists
void build_all_zonelists(void)
    |---->set_zonelist_order()
         |---->current_zonelist_order = ZONELIST_ORDER_ZONE;
    |
    |---->__build_all_zonelists(NULL);
    |    Memory不支持热插拔, 为每个zone建立后备的zone,
    |    每个zone及自己后备的zone，形成zonelist
        |
        |---->pg_data_t *pgdat = NULL;
        |     pgdat = &contig_page_data;(单node)
        |
        |---->build_zonelists(pgdat);
        |     为每个zone建立后备zone的列表
            |
            |---->struct zonelist *zonelist = NULL;
            |     enum zone_type j;
            |     zonelist = &pgdat->node_zonelists[0];
            |
            |---->j = build_zonelists_node(pddat, zonelist, 0, MAX_NR_ZONES - 1);
            |     为pgdat->node_zones[0]建立后备的zone，node_zones[0]后备的zone
            |     存储在node_zonelist[0]内，对于node_zone[0]的后备zone，其后备的zone
            |     链表如下(只考虑UMA体系，而且不考虑ZONE_DMA)：
            |     node_zonelist[0]._zonerefs[0].zone = &node_zones[2];
            |     node_zonelist[0]._zonerefs[0].zone_idx = 2;
            |     node_zonelist[0]._zonerefs[1].zone = &node_zones[1];
            |     node_zonelist[0]._zonerefs[1].zone_idx = 1;
            |     node_zonelist[0]._zonerefs[2].zone = &node_zones[0];
            |     node_zonelist[0]._zonerefs[2].zone_idx = 0;
            |
            |     zonelist->_zonerefs[3].zone = NULL;
            |     zonelist->_zonerefs[3].zone_idx = 0;
        |
        |---->build_zonelist_cache(pgdat);
              |---->pdat->node_zonelists[0].zlcache_ptr = NULL;
              |     UMA体系结构
              |
        |---->for_each_possible_cpu(cpu)
        |     setup_pageset(&per_cpu(boot_pageset, cpu), 0);
              |详见下文
    |---->vm_total_pages = nr_free_pagecache_pages();
    |    业务：获得所有zone中的present_pages总和.
    |
    |---->page_group_by_mobility_disabled = 0;
    |     对于代码中的判断条件一般不会成立，因为页数会最够多（内存较大）
